# Model-Quantization-Hands-On
This repo represent the hands-on model quantization for better performance and minimize the loss of accuracy

## Disclaimer

The content and examples in this repository are based on the author's current knowledge and experience. The techniques and practices demonstrated here reflect common approaches that the author frequently uses and may not cover all possible methods or the latest advancements in model quantization. Users are encouraged to consult additional resources and exercise their own judgment when applying these methods to their own projects.

## bitsandbytes

Target model type: LLM
[See details in bitsandbytes/README.md](./bitsandbytes/README.md)

## AWQ

Target model type: LLM
[See details in AWQ/README.md](./AWQ/README.md)

## GPTQ

Target model type: LLM
[See details in GPTQ/README.md](./GPTQ/README.md)